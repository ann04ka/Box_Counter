version: '3.8'

services:
  model_inference:
    build: .
    container_name: model_inference

    volumes:
      - ./models:/app/models            # Папка с весами
      - ./images:/app/input             # Входные изображения
      - ./results:/app/output           # Папка для результатов
      - ./inference.py:/app/inference.py  # Скрипт инференса

    working_dir: /app

    environment:
      - PYTHONPATH=/app
      - TORCH_HOME=/app/models/.torch  # Кэш PyTorch моделей

    tty: true
    stdin_open: true

    # Для batch инференса:
    command: ["python", "inference.py", "--input", "/app/input", "--output", "/app/output"]

    # Ограничения ресурсов (опционально)
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '4.0'
          memory: 8G
